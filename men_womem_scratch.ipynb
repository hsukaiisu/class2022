{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "men_womem_scratch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kCI5ElkMPCozAeqDZ4Q79K9jP7EKD073",
      "authorship_tag": "ABX9TyOAEAbWXaBo72FxounlUItt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluecityisu/class2022/blob/main/men_womem_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "dNFzBGf_jfKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "jujdZiM8tcB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "6IFQmdADwFix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "eu24nYe9tghp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be augmented\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/class/202209/pic/train',  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/class/202209/pic/val',\n",
        "        target_size=(150, 150),\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "rgwJhrOHtmTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_generator,  \n",
        "      epochs=15,\n",
        "      validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "AB-UIhvetn-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/class/202209/tmp/rps.h5\")"
      ],
      "metadata": {
        "id": "oCnBZPcM3CyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('/content/drive/MyDrive/class/202209/tmp/rps.h5')"
      ],
      "metadata": {
        "id": "sUI-SoAe7w7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "img_height=150\n",
        "img_width=150\n",
        "#img_path = r'/content/drive/MyDrive/class/202209/pic/train/women/00000013.jpg'\n",
        "#img_path = r'/content/drive/MyDrive/class/202209/pic/train/men/00000021.jpg'\n",
        "img_path=r'/content/drive/MyDrive/class/202209/yang.jpg'\n",
        "img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor1 = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor1 = img_tensor1/255.0\n",
        "img_tensor2 = np.vstack([img_tensor1])\n",
        "\n",
        "featuremap = new_model.predict(img_tensor2)\n",
        "plt.imshow(img)\n",
        "print(np.around(featuremap,3))\n",
        "print(featuremap.argmax())\n"
      ],
      "metadata": {
        "id": "HaiuPU7X4ynu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}